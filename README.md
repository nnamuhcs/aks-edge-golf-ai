# AKS Edge Golf AI â€“ Swing Analyzer

A production-grade, containerized golf swing analyzer powered by AI â€” built for **Azure Kubernetes Service (AKS)**, **AKS Arc**, and **AKS Edge Essentials**. Upload a golf swing video and get instant stage-by-stage feedback with annotated side-by-side comparisons against good practice references.

## Features

- ğŸ¥ **Video Upload** â€“ Drag & drop or browse; supports MP4, MOV, AVI, WebM
- ğŸ¤– **AI Analysis** â€“ MediaPipe pose estimation + CLIP embedding matching (all local, no cloud APIs)
- ğŸ“Š **8-Stage Breakdown** â€“ Address â†’ Takeaway â†’ Backswing â†’ Top â†’ Downswing â†’ Impact â†’ Follow-Through â†’ Finish
- ğŸ¯ **Per-Stage Scoring** â€“ 0â€“100 score with detailed good/bad/why/tips feedback
- ğŸ–¼ï¸ **Side-by-Side Comparison** â€“ Annotated user vs. reference frames with skeleton overlays and callouts
- ğŸ” **Click to Enlarge** â€“ Lightbox for detailed frame inspection
- â˜¸ï¸ **AKS Ready** â€“ Deploy to AKS, AKS Arc, AKS Edge Essentials, or any conformant K8s cluster
- ğŸ“ **Architecture Viewer** â€“ Interactive system architecture diagram built into the UI
- ğŸ“¡ **Live K8s Panel** â€“ Real-time cluster status when running in Kubernetes

## Quick Start

### Prerequisites

| Tool | Version | Install |
|------|---------|---------|
| kubectl | 1.28+ | [kubernetes.io](https://kubernetes.io/docs/tasks/tools/) |
| Azure CLI | 2.50+ | [learn.microsoft.com](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) |

### Deploy to AKS (Azure Kubernetes Service)

```bash
# 1. Clone the repo
git clone https://github.com/nnamuhcs/aks-edge-golf-ai.git
cd aks-edge-golf-ai

# 2. Connect to your AKS cluster
az login
az aks get-credentials --name <your-cluster> --resource-group <your-rg>

# 3. Deploy â€” pre-built images pull from ghcr.io automatically
kubectl apply -k deploy/base/

# 4. Expose the frontend via LoadBalancer
kubectl patch svc golf-frontend -n golf-ai -p '{"spec":{"type":"LoadBalancer"}}'

# 5. Get the external IP (wait ~1 min for Azure to assign it)
kubectl get svc golf-frontend -n golf-ai -w
```

Open **http://\<EXTERNAL-IP\>** once the IP appears.

> â³ **First deploy:** The backend image is ~6GB (includes all ML models baked in). Initial pull takes **3â€“10 minutes** depending on node size and network speed. Monitor with `kubectl get pods -n golf-ai -w`.

### Deploy to AKS Arc / AKS Edge Essentials

```bash
# 1. Connect to your AKS Arc cluster (on Azure Stack HCI / Azure Local)
az login
az connectedk8s proxy --name <arc-cluster> --resource-group <your-rg>
# Or use the kubeconfig from your on-prem cluster directly

# 2. Deploy
kubectl apply -k deploy/base/

# 3. Access via NodePort
kubectl get svc golf-frontend -n golf-ai
# Frontend is exposed on NodePort 30080
```

Open **http://\<node-ip\>:30080**

> ğŸ’¡ **AKS Edge Essentials:** Same steps apply â€” the manifests work on any K3s/K8s cluster provisioned by AKS Edge. Ensure your node has at least **8GB RAM** for the ML models.

### Build Images Yourself (Optional)

If you prefer to build from source or push to a private registry:

```bash
# Build (backend ~5 min â€” downloads ML models during build)
docker build -t golf-ai-backend:latest -f backend/Dockerfile backend/
docker build -t golf-ai-frontend:latest -f frontend/Dockerfile frontend/

# Push to your ACR
az acr login --name <yourregistry>
docker tag golf-ai-backend:latest <yourregistry>.azurecr.io/golf-ai-backend:latest
docker tag golf-ai-frontend:latest <yourregistry>.azurecr.io/golf-ai-frontend:latest
docker push <yourregistry>.azurecr.io/golf-ai-backend:latest
docker push <yourregistry>.azurecr.io/golf-ai-frontend:latest

# Attach ACR to AKS and deploy with your registry
az aks update --name <cluster> --resource-group <rg> --attach-acr <yourregistry>
kubectl apply -k deploy/overlays/demo   # Edit overlay to point to your registry
```

> ğŸ“– **Full deployment guide** with troubleshooting and configuration: [docs/deployment-guide.md](docs/deployment-guide.md)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend       â”‚    â”‚   Backend (FastAPI)               â”‚
â”‚   (React/Vite)   â”‚â”€â”€â”€â–¶â”‚                                  â”‚
â”‚                  â”‚    â”‚  POST /api/upload                 â”‚
â”‚  Upload Panel    â”‚    â”‚  GET  /api/status/{job_id}        â”‚
â”‚  Progress Bar    â”‚    â”‚  GET  /api/result/{job_id}        â”‚
â”‚  Results Panel   â”‚    â”‚  GET  /assets/...                 â”‚
â”‚  - Stage Timelineâ”‚    â”‚                                  â”‚
â”‚  - Side-by-Side  â”‚    â”‚  Pipeline:                       â”‚
â”‚  - Feedback      â”‚    â”‚  1. Frame Extraction (OpenCV)    â”‚
â”‚  - Lightbox      â”‚    â”‚  2. Pose Detection (MediaPipe)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  3. Stage Segmentation           â”‚
                        â”‚  4. Orientation Normalization    â”‚
                        â”‚  5. Body Metrics Computation     â”‚
                        â”‚  6. Scoring + Feedback           â”‚
                        â”‚  7. Reference Matching (CLIP/HF) â”‚
                        â”‚  8. Frame Annotation             â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Model Choices

| Model | Purpose | Source |
|-------|---------|--------|
| MediaPipe Pose | Body landmark detection | Google MediaPipe |
| CLIP ViT-B/32 | Embedding similarity for reference matching | HuggingFace (openai/clip-vit-base-patch32) |

Both models run on CPU with acceptable latency for demo purposes. CLIP is downloaded at Docker build time and cached.

## Project Structure

```
aks-edge-golf-ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/                 # FastAPI application
â”‚   â”‚   â”œâ”€â”€ main.py          # API endpoints
â”‚   â”‚   â”œâ”€â”€ pipeline.py      # Analysis orchestrator
â”‚   â”‚   â”œâ”€â”€ video_decoder.py # Frame extraction
â”‚   â”‚   â”œâ”€â”€ pose_estimator.py# MediaPipe pose
â”‚   â”‚   â”œâ”€â”€ stage_segmentation.py
â”‚   â”‚   â”œâ”€â”€ orientation.py   # Frame rotation fix
â”‚   â”‚   â”œâ”€â”€ scoring.py       # Metrics + feedback
â”‚   â”‚   â”œâ”€â”€ annotator.py     # Visual annotations
â”‚   â”‚   â”œâ”€â”€ reference_matcher.py # CLIP matching
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ tests/               # pytest tests
â”‚   â”œâ”€â”€ reference_data/      # Good-practice frames
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadPanel.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ProgressPanel.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ResultsPanel.jsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ base/                # K8s base manifests
â”‚   â””â”€â”€ overlays/demo/       # Demo kustomize overlay
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_demo_content.py
â”‚   â””â”€â”€ verify_and_fix.sh
â”œâ”€â”€ sample_videos/           # Demo videos (generated)
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ Makefile
```

## Testing

```bash
# Run backend tests
make test

# Run full verification loop (lint + test + build)
make verify
```

## How Stage Segmentation Works

The segmentation algorithm uses pose landmark signals:
1. **Motion signal** â€“ velocity of key joints (wrists, hips, shoulders) frame-to-frame
2. **Wrist height signal** â€“ proxy for club position (higher wrists = backswing/top)

Key transition points:
- **Address**: minimal motion at start
- **Top**: wrist height minimum (highest point in image coords)
- **Impact**: peak motion velocity
- Other stages are interpolated between these anchor points

**Limitations**: Works best with side-view videos. May struggle with oblique angles, multiple people, or very short clips.

## Adding New Reference Swings

1. Place stage frames in `backend/reference_data/stages/<stage_name>/`
2. Name files `ref_01.png`, `ref_02.png`, etc.
3. Rebuild the backend container or restart the server
4. The system will compute CLIP embeddings on startup and use the best-matching reference for each user stage

## License

MIT License. See [docs/licenses.md](docs/licenses.md) for dependency licenses.
