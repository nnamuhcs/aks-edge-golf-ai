# AKS Edge Golf AI â€“ Swing Analyzer

A production-grade, containerized golf swing analyzer powered by AI. Upload a video, get instant stage-by-stage feedback with annotated side-by-side comparisons against good practice references.

## Features

- ğŸ¥ **Video Upload** â€“ Drag & drop or browse; supports MP4, MOV, AVI, WebM
- ğŸ¤– **AI Analysis** â€“ MediaPipe pose estimation + CLIP embedding matching
- ğŸ“Š **8-Stage Breakdown** â€“ Address â†’ Takeaway â†’ Backswing â†’ Top â†’ Downswing â†’ Impact â†’ Follow-Through â†’ Finish
- ğŸ¯ **Per-Stage Scoring** â€“ 0â€“100 score with detailed good/bad/why/tips feedback
- ğŸ–¼ï¸ **Side-by-Side Comparison** â€“ Annotated user vs. reference frames with skeleton overlays and callouts
- ğŸ” **Click to Enlarge** â€“ Lightbox for detailed frame inspection
- â˜¸ï¸ **K8s Ready** â€“ Deploy to AKS, Kind, or any conformant K8s cluster
- ğŸ“ **Architecture Viewer** â€“ Interactive system architecture diagram built into the UI
- ğŸ“¡ **Live K8s Panel** â€“ Real-time cluster status when running in Kubernetes

## Quick Start (Local â€” No Docker)

```bash
git clone https://github.com/nnamuhcs/aks-edge-golf-ai.git
cd aks-edge-golf-ai

# Backend
cd backend
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8000
# First run downloads CLIP model (~600MB) from HuggingFace

# Frontend (separate terminal)
cd frontend
npm install && npm run build
# Backend serves the built frontend at http://localhost:8000
```

Open **http://localhost:8000**

## Quick Start (Kind â€” Local Kubernetes)

**Option A: Pull pre-built images (no build needed)**

```bash
# Create Kind cluster with port mapping
kind create cluster --name golf-ai --config deploy/kind-config.yaml

# Deploy â€” images pull from ghcr.io automatically
kubectl apply -k deploy/base/
```

> â³ **Note:** The backend image is ~6GB (includes ML models). First pull may take 5â€“15 minutes depending on your internet speed. You can monitor progress with `kubectl get pods -n golf-ai -w`.

**Option B: Build images locally**

```bash
# Build images (backend ~5 min first time â€” downloads ML models)
docker build -t golf-ai-backend:latest -f backend/Dockerfile backend/
docker build -t golf-ai-frontend:latest -f frontend/Dockerfile frontend/

# Create Kind cluster with port mapping
kind create cluster --name golf-ai --config deploy/kind-config.yaml

# Load locally-built images into Kind & deploy
kind load docker-image golf-ai-backend:latest golf-ai-frontend:latest --name golf-ai
kubectl apply -k deploy/overlays/kind
```

Open **http://localhost:3001** â€” no port-forward needed!

> ğŸ“– **Full deployment guide** (including AKS): [docs/deployment-guide.md](docs/deployment-guide.md)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend       â”‚    â”‚   Backend (FastAPI)               â”‚
â”‚   (React/Vite)   â”‚â”€â”€â”€â–¶â”‚                                  â”‚
â”‚                  â”‚    â”‚  POST /api/upload                 â”‚
â”‚  Upload Panel    â”‚    â”‚  GET  /api/status/{job_id}        â”‚
â”‚  Progress Bar    â”‚    â”‚  GET  /api/result/{job_id}        â”‚
â”‚  Results Panel   â”‚    â”‚  GET  /assets/...                 â”‚
â”‚  - Stage Timelineâ”‚    â”‚                                  â”‚
â”‚  - Side-by-Side  â”‚    â”‚  Pipeline:                       â”‚
â”‚  - Feedback      â”‚    â”‚  1. Frame Extraction (OpenCV)    â”‚
â”‚  - Lightbox      â”‚    â”‚  2. Pose Detection (MediaPipe)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  3. Stage Segmentation           â”‚
                        â”‚  4. Orientation Normalization    â”‚
                        â”‚  5. Body Metrics Computation     â”‚
                        â”‚  6. Scoring + Feedback           â”‚
                        â”‚  7. Reference Matching (CLIP/HF) â”‚
                        â”‚  8. Frame Annotation             â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Model Choices

| Model | Purpose | Source |
|-------|---------|--------|
| MediaPipe Pose | Body landmark detection | Google MediaPipe |
| CLIP ViT-B/32 | Embedding similarity for reference matching | HuggingFace (openai/clip-vit-base-patch32) |

Both models run on CPU with acceptable latency for demo purposes. CLIP is downloaded at Docker build time and cached.

## Project Structure

```
aks-edge-golf-ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/                 # FastAPI application
â”‚   â”‚   â”œâ”€â”€ main.py          # API endpoints
â”‚   â”‚   â”œâ”€â”€ pipeline.py      # Analysis orchestrator
â”‚   â”‚   â”œâ”€â”€ video_decoder.py # Frame extraction
â”‚   â”‚   â”œâ”€â”€ pose_estimator.py# MediaPipe pose
â”‚   â”‚   â”œâ”€â”€ stage_segmentation.py
â”‚   â”‚   â”œâ”€â”€ orientation.py   # Frame rotation fix
â”‚   â”‚   â”œâ”€â”€ scoring.py       # Metrics + feedback
â”‚   â”‚   â”œâ”€â”€ annotator.py     # Visual annotations
â”‚   â”‚   â”œâ”€â”€ reference_matcher.py # CLIP matching
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ tests/               # pytest tests
â”‚   â”œâ”€â”€ reference_data/      # Good-practice frames
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadPanel.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ProgressPanel.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ResultsPanel.jsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ base/                # K8s base manifests
â”‚   â””â”€â”€ overlays/demo/       # Demo kustomize overlay
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_demo_content.py
â”‚   â””â”€â”€ verify_and_fix.sh
â”œâ”€â”€ sample_videos/           # Demo videos (generated)
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ Makefile
```

## Testing

```bash
# Run backend tests
make test

# Run full verification loop (lint + test + build)
make verify
```

## How Stage Segmentation Works

The segmentation algorithm uses pose landmark signals:
1. **Motion signal** â€“ velocity of key joints (wrists, hips, shoulders) frame-to-frame
2. **Wrist height signal** â€“ proxy for club position (higher wrists = backswing/top)

Key transition points:
- **Address**: minimal motion at start
- **Top**: wrist height minimum (highest point in image coords)
- **Impact**: peak motion velocity
- Other stages are interpolated between these anchor points

**Limitations**: Works best with side-view videos. May struggle with oblique angles, multiple people, or very short clips.

## Adding New Reference Swings

1. Place stage frames in `backend/reference_data/stages/<stage_name>/`
2. Name files `ref_01.png`, `ref_02.png`, etc.
3. Rebuild the backend container or restart the server
4. The system will compute CLIP embeddings on startup and use the best-matching reference for each user stage

## License

MIT License. See [docs/licenses.md](docs/licenses.md) for dependency licenses.
