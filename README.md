# AKS Edge Golf AI â€“ Swing Analyzer

A production-grade, containerized golf swing analyzer powered by AI. Upload a video, get instant stage-by-stage feedback with annotated side-by-side comparisons against good practice references.

![Demo](docs/demo-screenshot.png)

## Features

- ğŸ¥ **Video Upload** â€“ Drag & drop or browse; supports MP4, MOV, AVI, WebM
- ğŸ¤– **AI Analysis** â€“ MediaPipe pose estimation + CLIP embedding matching
- ğŸ“Š **8-Stage Breakdown** â€“ Address â†’ Takeaway â†’ Backswing â†’ Top â†’ Downswing â†’ Impact â†’ Follow-Through â†’ Finish
- ğŸ¯ **Per-Stage Scoring** â€“ 0â€“100 score with detailed good/bad/why/tips feedback
- ğŸ–¼ï¸ **Side-by-Side Comparison** â€“ Annotated user vs. reference frames with skeleton overlays and callouts
- ğŸ” **Click to Enlarge** â€“ Lightbox for detailed frame inspection
- â˜¸ï¸ **K8s Ready** â€“ Deploy to AKS or any conformant cluster
- ğŸ³ **Docker Compose** â€“ One-command local demo

## Quick Start

### Prerequisites
- Python 3.11+
- Node.js 18+
- Docker (optional)

### Local Development

```bash
# 1. Generate demo content (synthetic videos + reference frames)
make demo-content

# 2. Install dependencies
make setup

# 3. Start backend (terminal 1)
make backend

# 4. Start frontend (terminal 2)
make frontend

# 5. Open http://localhost:3000
```

### Docker Compose (Recommended)

```bash
# Generate demo content first
make demo-content

# Build and run
docker-compose up --build

# Open http://localhost:3000
```

### Kubernetes Deployment

```bash
# Build images
make build

# Tag for your registry (if needed)
docker tag golf-swing-ai-backend <registry>/golf-swing-ai-backend:latest
docker tag golf-swing-ai-frontend <registry>/golf-swing-ai-frontend:latest
docker push <registry>/golf-swing-ai-backend:latest
docker push <registry>/golf-swing-ai-frontend:latest

# Deploy
kubectl apply -k deploy/overlays/demo

# Port-forward for access
make port-forward

# Open http://localhost:3000
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend       â”‚    â”‚   Backend (FastAPI)               â”‚
â”‚   (React/Vite)   â”‚â”€â”€â”€â–¶â”‚                                  â”‚
â”‚                  â”‚    â”‚  POST /api/upload                 â”‚
â”‚  Upload Panel    â”‚    â”‚  GET  /api/status/{job_id}        â”‚
â”‚  Progress Bar    â”‚    â”‚  GET  /api/result/{job_id}        â”‚
â”‚  Results Panel   â”‚    â”‚  GET  /assets/...                 â”‚
â”‚  - Stage Timelineâ”‚    â”‚                                  â”‚
â”‚  - Side-by-Side  â”‚    â”‚  Pipeline:                       â”‚
â”‚  - Feedback      â”‚    â”‚  1. Frame Extraction (OpenCV)    â”‚
â”‚  - Lightbox      â”‚    â”‚  2. Pose Detection (MediaPipe)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  3. Stage Segmentation           â”‚
                        â”‚  4. Orientation Normalization    â”‚
                        â”‚  5. Body Metrics Computation     â”‚
                        â”‚  6. Scoring + Feedback           â”‚
                        â”‚  7. Reference Matching (CLIP/HF) â”‚
                        â”‚  8. Frame Annotation             â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Model Choices

| Model | Purpose | Source |
|-------|---------|--------|
| MediaPipe Pose | Body landmark detection | Google MediaPipe |
| CLIP ViT-B/32 | Embedding similarity for reference matching | HuggingFace (openai/clip-vit-base-patch32) |

Both models run on CPU with acceptable latency for demo purposes. CLIP is downloaded at Docker build time and cached.

## Project Structure

```
aks-edge-golf-ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/                 # FastAPI application
â”‚   â”‚   â”œâ”€â”€ main.py          # API endpoints
â”‚   â”‚   â”œâ”€â”€ pipeline.py      # Analysis orchestrator
â”‚   â”‚   â”œâ”€â”€ video_decoder.py # Frame extraction
â”‚   â”‚   â”œâ”€â”€ pose_estimator.py# MediaPipe pose
â”‚   â”‚   â”œâ”€â”€ stage_segmentation.py
â”‚   â”‚   â”œâ”€â”€ orientation.py   # Frame rotation fix
â”‚   â”‚   â”œâ”€â”€ scoring.py       # Metrics + feedback
â”‚   â”‚   â”œâ”€â”€ annotator.py     # Visual annotations
â”‚   â”‚   â”œâ”€â”€ reference_matcher.py # CLIP matching
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ tests/               # pytest tests
â”‚   â”œâ”€â”€ reference_data/      # Good-practice frames
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadPanel.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ProgressPanel.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ResultsPanel.jsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ base/                # K8s base manifests
â”‚   â””â”€â”€ overlays/demo/       # Demo kustomize overlay
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_demo_content.py
â”‚   â””â”€â”€ verify_and_fix.sh
â”œâ”€â”€ sample_videos/           # Demo videos (generated)
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ Makefile
```

## Testing

```bash
# Run backend tests
make test

# Run full verification loop (lint + test + build)
make verify
```

## How Stage Segmentation Works

The segmentation algorithm uses pose landmark signals:
1. **Motion signal** â€“ velocity of key joints (wrists, hips, shoulders) frame-to-frame
2. **Wrist height signal** â€“ proxy for club position (higher wrists = backswing/top)

Key transition points:
- **Address**: minimal motion at start
- **Top**: wrist height minimum (highest point in image coords)
- **Impact**: peak motion velocity
- Other stages are interpolated between these anchor points

**Limitations**: Works best with side-view videos. May struggle with oblique angles, multiple people, or very short clips.

## Adding New Reference Swings

1. Place stage frames in `backend/reference_data/stages/<stage_name>/`
2. Name files `ref_01.png`, `ref_02.png`, etc.
3. Rebuild the backend container or restart the server
4. The system will compute CLIP embeddings on startup and use the best-matching reference for each user stage

## License

MIT License. See [docs/licenses.md](docs/licenses.md) for dependency licenses.
